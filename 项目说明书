一、概述
    该项目为数据仓库管理的项目，模拟电子商品平台进行数据采集、清洗、分析和存储等操作，进而对数据进行分析和统计。其中，日志分为启动日志和行为日志两大类。
    1. 启动日志格式为JSON字符，并且{"en":"start"},如下
    {
    	"action": "2",
    	"ar": "MX",
    	"ba": "Sumsung",
    	"detail": "",
    	"en": "start",
    	"entry": "1",
    	"extend1": "",
    	"g": "1C6M27BZ@gmail.com",
    	"hw": "750*1134",
    	"l": "es",
    	"la": "-36.0",
    	"ln": "-78.9",
    	"loading_time": "9",
    	"md": "sumsung-3",
    	"mid": "0",
    	"nw": "3G",
    	"open_ad_type": "2",
    	"os": "8.1.1",
    	"sr": "G",
    	"sv": "V2.9.5",
    	"t": "1557841766453",
    	"uid": "0",
    	"vc": "4",
    	"vn": "1.0.0"
    }

    2.行为日志格式为："共同属性+时间戳 + | + 行为集"用户操作基础信息等，如下
    1557883491565 | {
    	"cm": {
    		"ln": "-80.9",
    		"sv": "V2.2.8",
    		"os": "8.0.8",
    		"g": "U5HOH584@gmail.com",
    		"mid": "37",
    		"nw": "WIFI",
    		"l": "en",
    		"vc": "18",
    		"hw": "1080*1920",
    		"ar": "MX",
    		"uid": "37",
    		"t": "1557796007093",
    		"la": "-10.4",
    		"md": "Huawei-18",
    		"vn": "1.1.5",
    		"ba": "Huawei",
    		"sr": "A"
    	},
    	"ap": "gmall",
    	"et": [{
    		"ett": "1557841713573",
    		"en": "loading",
    		"kv": {
    			"extend2": "",
    			"loading_time": "40",
    			"action": "1",
    			"extend1": "",
    			"type": "2",
    			"type1": "",
    			"loading_way": "2"
    		}
    	}, {
    		"ett": "1557828739181",
    		"en": "notification",
    		"kv": {
    			"ap_time": "1557854867024",
    			"action": "1",
    			"type": "3",
    			"content": ""
    		}
    	}, {
    		"ett": "1557793047938",
    		"en": "active_foreground",
    		"kv": {
    			"access": "1",
    			"push_id": "1"
    		}
    	}, {
    		"ett": "1557797333985",
    		"en": "favorites",
    		"kv": {
    			"course_id": 3,
    			"id": 0,
    			"add_time": "1557866826036",
    			"userid": 4
    		}
    	}]
    }

二、项目需求以及架构设计
    1) 明确项目需求
       (1) 数据采集平台搭建
       (2) 实现用户行为数据仓库的分层搭建
       (3) 实现业务数据仓库的分层搭建
       (4) 针对数据仓库中的数据进行，留存、转化率、GMV、复购率、活跃等报表分析
    2) 项目环境准备
       (1) 项目的技术如何选型(根据自己熟悉的技术进行选型,至少选出两套技术方案)
            ① 数据采集传输: Flume、Kafka、Sqoop、Logstash、DataX
            ② 数据存储：MySQL、HDFS、HBase、Redis、MongoDB(一般用于爬虫数据存储)
            ③ 数据计算：Hive、Tez、Spark、Flink、Storm  -> (在计算周、日、月这种周期比较长的数据时,应该选Hive)
            ④ 数据查询：Presto、Druid、Impala、Kylin
            ⑤ 选好技术,设计项目流程图
       (2) 框架版本如何选型(Apache、CDH、HDP)
            ① Apache：运维麻烦,组件间兼容性需要自己调研(一般大公司使用,技术实力雄厚,有专业的运维人员)
            ② CDH: 国内使用最多的版本,但CM不开源,但对于中、小型公司来说使用并不会影响(建议使用)
            ③ HDP：开源,可以进行二次开发，但是没有CDH稳定,国内使用较少
       (3) 服务器使用物理机还是云主机

       (4) 如何确认集群规模?(假设每台服务器的硬盘容量是8T)

二、业务流程(其中Kafka重要作用：1.削峰 2.为以后实时做好准备)
    日志文件 -> Flume -> Kafka -> Flume -> HDFS -> Hive(分层：ODS -> DWD -> DWS -> ADS) -> 用户画像、报表系统等

三、将data-ware-house-mock打包并且上传到服务器，并且执行以下命令进行模拟产生数据(每隔2秒产生一条数据,总共产生10条)
    java -classpath /opt/project/data-ware-house/data-ware-house-mock.jar com.yxBuild.ProductDataApp 2 10

四、搭建集群

五、配置第一层Flume采集数据
    1) Agent选型
        Source类型为TailDir Source(注：Flume 1.7提供的Source组件，在1.6中并没有)
        Channel类型为KafkaChannel(好处：省去Sink组件,并且提高数据的吞吐量、安全性)

    2) 配置第一个Agent(agent-tailDir-kafka.conf),进行接收数据并且保存到KafkaChannel中
        (1) 配置Source
            # 声明并且配置Source
            a1.sources=r1
            a1.sources.r1.type = TAILDIR
            a1.sources.r1.positionFile = /opt/project/data-ware-house/log_position.json
            a1.sources.r1.filegroups = f1
            a1.sources.r1.filegroups.f1 = /opt/project/data-ware-house/data/app.+
            a1.sources.r1.fileHeader = true

        (2) 定义拦截器进行对数据过滤
            ① 清洗拦截器(启动日志的格式是否符合JSON、行为日志的时间戳和数据格式是否正确)
                com.yxBuild.interceptor.LogETLInterceptor

            ② 日志类型拦截器(根据启动日志和行为日志进行添加Header值,从而设置Kafka Channel进行对应的Topic进行生产数据)
                com.yxBuild.interceptor.LogTypeInterceptor

            ③ 将编写好的拦截器项目编译成Jar并且上传到Flume的lib文件夹下

            ④ 在Agent配置文件夹中定义并且关联拦截器
                # Resource关联自定义的拦截器
                a1.sources.r1.interceptors =  i1 i2
                a1.sources.r1.interceptors.i1.type = com.yxBuild.interceptor.LogETLInterceptor$Builder
                a1.sources.r1.interceptors.i2.type = com.yxBuild.interceptor.LogTypeInterceptor$Builder

        (3) 在Agent配置文件中定义Channel Selector(Channel选择器)并且进行配置
            # 配置Source关联Channel选择器
            a1.sources.r1.selector.type = multiplexing
            a1.sources.r1.selector.header = topic
            a1.sources.r1.selector.mapping.topic_start = c1
            a1.sources.r1.selector.mapping.topic_event = c2

        (4) 配置Channel并且与Source进行关联
            # 声明并且配置Channel
            a1.channels=c1 c2
            a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
            a1.channels.c1.kafka.bootstrap.servers = hadoop103:9092,hadoop104:9092,hadoop105:9092
            a1.channels.c1.kafka.topic = topic_start
            a1.channels.c1.parseAsFlumeEvent = false
            a1.channels.c1.kafka.consumer.group.id = flume-consumer

            a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
            a1.channels.c2.kafka.bootstrap.servers = hadoop103:9092,hadoop104:9092,hadoop105:9092
            a1.channels.c2.kafka.topic = topic_event
            a1.channels.c2.parseAsFlumeEvent = false
            a1.channels.c2.kafka.consumer.group.id = flume-consumer

            # Source关联Channel
            a1.sources.r1.channels = c1 c2

    3) 将脚本Agent的agent-tailDir-kafka.conf上传到服务器并且进行启动
        1) 在Kafka创建相关的主题topic_start、topic_event

        2) 启动Agent(agent-tailDir-kafka.conf)脚本
        /opt/module/flume-1.7.0/bin/flume-ng agent --conf /opt/module/flume-1.7.0/conf/ --name a1 --conf-file agent-tailDir-kafka.conf



六、配置第二层Flume采集数据(agent-kafka-file-hdfs.conf)
    1) 模型: Kafka Source(两个Source) -> File Channel(两个File Channel) -> HDFS Sink(两个Sink输出)
        Source: Kafka Source
        Channel: File Channel
        Sink: HDFS Sink

    2) 为了防止Flume输出产生大量的小文件
        ① 配置Sink10分钟和文件大小为128M时候进行滚动输出一次

七、创建原始层数据表
    1）启动日志原始层数据表
        create external table ods_start_log(
        line string
        )
        partitioned by (dt string)
        stored as
          inputformat 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
          outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
        location '/project/data-ware-house/origin_data/gmall/hive/ods/ods_start_log';
    2）行为日志原始层数据表
        create external table ods_event_log(
        line string
        )
        partitioned by (dt string)
        stored as
          inputformat 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
          outputformat 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
        location '/project/data-ware-house/origin_data/gmall/hive/ods/ods_event_log';
     3）导入数据
        ① 开始日志的原始层
            load data inpath '/project/data-ware-house/origin_data/gmall/log/topic_start/2019-05-16' into table  ods_start_log partition(dt='2019-05-16');

        ② 启动日志的原始层
            load data inpath '/project/data-ware-house/origin_data/gmall/log/topic_event/2019-05-18' into table ods_event_log partition(dt = '2019-05-18');

        提出疑问：在第二层Flume中,如果将数据动态分区的将数据导入到Hive中?
            解决思路：编写Shell脚本,获取当前日期,从而执行load data 命令

八、创建数据详细层---对原始数据层的数据进行清洗
    1） 启动日志详细表(dwd_start_log)---对原始层的数据表的JSON的字段值进行切分保存
        ① 创建启动日志详细表，使用get_json_object函数,将line字段值对象值进行获取并保存

    2）对行为数据进行分类保存到数据表
        ① 创建事件基础表(dwd_base_event_log),保存共同字段信息、事件名称、事件的JSON数据

        ② 根据事件基础表中的事件名称,进行将事件数据进行归类,并且保存导行为详细数据表中

        ③
    问题记录：
        1）如何定义UDF函数?定义UDF函数的作用是什么?
            UDF函数是一进一出函数,即是一条记录进去返回一行信息,定义UDF函数需要继承Hive的UDF类,并且重载它的evaluate方法,
            在数仓项目中定义,UDF主要是解析行为日志的共同部分的数据,从而以制表符作为分割符合返回,使用Hive自带的Split函数
            进一步获取每个属性值
        2) 如何定义UDTF函数?在项目中所用的目的是什么?
            UDTF为一进多出函数,自定义UDTF函数需要继承GenericUDTF类,并且重写initialize、process、close方法;在项目中主要
            处理用户行为日志,解析行为日志的数据,获取每个行为日志的名称
        3) 将自定义的UDF、UDTF函数进行打包,并且上传到服务器的Hive安装目录中的lib目录下

        4) 将自定义的函数进行临时关联
            ① 将函数JAR加入到Hive的classpath中
                hive (gmall2)> add jar /opt/module/hive-1.2.1/lib/data-ware-house-hive-1.0-SNAPSHOT.jar;
                hive (gmall2)>  create temporary function base_analizer as 'com.yxBuild.UDF.BaseFieldUDF';
                hive (gmall2)>  create temporary function flat_analizer as 'com.yxBuild.UDTF.EventJsonUDTF';
                注： 创建永久函数,去掉temporary执行即可
        5) 将原始层的行为日志,解析导入到DWD层


