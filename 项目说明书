一、概述
    该项目为数据仓库管理的项目，模拟电子商品平台进行数据采集、清洗、分析和存储等操作，进而对数据进行分析和统计。其中，日志分为启动日志和行为日志两大类。
    1. 启动日志格式为JSON字符，并且{"en":"start"},如下
    {"action":"2","ar":"MX","ba":"Sumsung","detail":"","en":"start","entry":"1","extend1":"","g":"1C6M27BZ@gmail.com","hw":"750*1134","l":"es","la":"-36.0","ln":"-78.9","loading_time":"9","md":"sumsung-3","mid":"0","nw":"3G","open_ad_type":"2","os":"8.1.1","sr":"G","sv":"V2.9.5","t":"1557841766453","uid":"0","vc":"4","vn":"1.0.0"}

    2.行为日志格式为：时间戳|用户操作基础信息等，如下
    {"action":"1","ar":"MX","ba":"Huawei","detail":"","en":"start","entry":"2","extend1":"","g":"43X82X63@gmail.com","hw":"640*1136","l":"en","la":"24.1","ln":"-41.5","loading_time":"15","md":"Huawei-14","mid":"2","nw":"4G","open_ad_type":"1","os":"8.2.9","sr":"T","sv":"V2.3.9","t":"1557784805521","uid":"2","vc":"4","vn":"1.0.3"}

二、业务流程(其中Kafka重要作用：1.削峰 2.为以后实时做好准备)
    日志文件 -> Flume -> Kafka -> Flume -> HDFS -> Hive(分层：ODS -> DWD -> DWS -> ADS) -> 用户画像、报表系统等

三、将data-ware-house-mock打包并且上传到服务器，并且执行以下命令进行模拟产生数据(每隔2秒产生一条数据,总共产生10条)
    java -classpath /opt/project/data-ware-house/data-ware-house-mock.jar com.yxBuild.ProductDataApp 2 10

四、搭建集群

五、配置第一层Flume采集数据
    1) Agent选型
        Source类型为TailDir Source(注：Flume 1.7提供的Source组件，在1.6中并没有)
        Channel类型为KafkaChannel(好处：省去Sink组件,并且提高数据的吞吐量、安全性)

    2) 配置第一个Agent(agent-tailDir-kafka.conf),进行接收数据并且保存到KafkaChannel中
        (1) 配置Source
            # 声明并且配置Source
            a1.sources=r1
            a1.sources.r1.type = TAILDIR
            a1.sources.r1.positionFile = /opt/project/data-ware-house/log_position.json
            a1.sources.r1.filegroups = f1
            a1.sources.r1.filegroups.f1 = /opt/project/data-ware-house/data/app.+
            a1.sources.r1.fileHeader = true

        (2) 定义拦截器进行对数据过滤
            ① 清洗拦截器(启动日志的格式是否符合JSON、行为日志的时间戳和数据格式是否正确)
                com.yxBuild.interceptor.LogETLInterceptor

            ② 日志类型拦截器(根据启动日志和行为日志进行添加Header值,从而设置Kafka Channel进行对应的Topic进行生产数据)
                com.yxBuild.interceptor.LogTypeInterceptor

            ③ 将编写好的拦截器项目编译成Jar并且上传到Flume的lib文件夹下

            ④ 在Agent配置文件夹中定义并且关联拦截器
                # Resource关联自定义的拦截器
                a1.sources.r1.interceptors =  i1 i2
                a1.sources.r1.interceptors.i1.type = com.yxBuild.interceptor.LogETLInterceptor$Builder
                a1.sources.r1.interceptors.i2.type = com.yxBuild.interceptor.LogTypeInterceptor$Builder

        (3) 在Agent配置文件中定义Channel Selector(Channel选择器)并且进行配置
            # 配置Source关联Channel选择器
            a1.sources.r1.selector.type = multiplexing
            a1.sources.r1.selector.header = topic
            a1.sources.r1.selector.mapping.topic_start = c1
            a1.sources.r1.selector.mapping.topic_event = c2

        (4) 配置Channel并且与Source进行关联
            # 声明并且配置Channel
            a1.channels=c1 c2
            a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
            a1.channels.c1.kafka.bootstrap.servers = hadoop103:9092,hadoop104:9092,hadoop105:9092
            a1.channels.c1.kafka.topic = topic_start
            a1.channels.c1.parseAsFlumeEvent = false
            a1.channels.c1.kafka.consumer.group.id = flume-consumer

            a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
            a1.channels.c2.kafka.bootstrap.servers = hadoop103:9092,hadoop104:9092,hadoop105:9092
            a1.channels.c2.kafka.topic = topic_event
            a1.channels.c2.parseAsFlumeEvent = false
            a1.channels.c2.kafka.consumer.group.id = flume-consumer

            # Source关联Channel
            a1.sources.r1.channels = c1 c2

    3) 将脚本Agent的agent-tailDir-kafka.conf上传到服务器并且进行启动
        1) 在Kafka创建相关的主题topic_start、topic_event

        2) 启动Agent(agent-tailDir-kafka.conf)脚本
        /opt/module/flume-1.7.0/bin/flume-ng agent --conf /opt/module/flume-1.7.0/conf/ --name a1 --conf-file agent-tailDir-kafka.conf


